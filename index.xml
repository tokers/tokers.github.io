<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>This is Alex Zhang</title>
    <link>https://tokers.github.io/</link>
    <description>Recent content on This is Alex Zhang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2019 11:47:45 +0800</lastBuildDate>
    
        <atom:link href="https://tokers.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LRU and LFU in Redis memory eviction</title>
      <link>https://tokers.github.io/posts/lru-and-lfu-in-redis-memory-eviction/</link>
      <pubDate>Mon, 27 May 2019 11:47:45 +0800</pubDate>
      
      <guid>https://tokers.github.io/posts/lru-and-lfu-in-redis-memory-eviction/</guid>
      <description>&lt;p&gt;Redis supports several memory eviction policies to limit the memory usage amounts. Two of them are the LRU based and LFU based algorithms, I want to write some stuffs to express my insights (or the source code analysis? Whatever.) about them. You can look through the bundled redis.conf file to learn all memory eviction policies details. It&amp;rsquo;s just my random note so feel free to rectify my faults!&lt;/p&gt;

&lt;p&gt;The LRU algorithm in Redis, is somewhat different from my intutive understanding. I have some Nginx modules development experiences, so I&amp;rsquo;m familiar with Nginx&amp;rsquo;s LRU use and implementation. Basically your node (object) should contain a field to mark its position in the LRU queue and whenever the node is accessed, it will be moved to the queue head, which means it was used recently. You might have guessed the eviction process: iterating objects from the tail of queue and free some of them. Frankly I think this pattern is good enough in most cases despite the queue is fat. The LRU implementation in Redis is totally different, each object only uses 24 bits (3 octets) to store the LRU clock, in contrast, the traditional LRU queue occupies at least 16 bytes(two pointers, &lt;code&gt;next&lt;/code&gt; and &lt;code&gt;prev&lt;/code&gt;). It&amp;rsquo;s space effective from this point of view, though 24 bits is narrow, it still can hold 194 days, and Redis handles overflow carefully. Each object&amp;rsquo;s LRU clock is updated when they are touched, to avoid too many system calls, the LRU clock was pre-computed with a settled frequency (&lt;code&gt;1000/server.hz&lt;/code&gt; ). Basically we use this pre-computed value is enough as long as the LRU clock resolution isn&amp;rsquo;t precise overly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;robj *lookupKey(redisDb, robj *key, int flags) {
    ...

    if (server.rdb_child_pid != -1 &amp;amp;&amp;amp;
        server.aof_child_pid != -1 &amp;amp;&amp;amp;
        !(flags &amp;amp; LOOKUP_NOTOUCH))
    {
         if (server.maxmemory_policy &amp;amp; MAXMEMORY_FLAG_LFU) {
             updateLFU(val);
         } else {
             val-&amp;gt;lru = LRU_CLOCK();
         }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;unsigned int LRU_CLOCK(void) {
    unsigned int lruclock;
    if (1000/server.hz &amp;lt;= LRU_CLOCK_RESOLUTION) {
        lruclock = server.lruclock;
    } else {
        lruclock = getLRUClock();
    }
    return lruclock;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;getLRUClock&lt;/code&gt; is trivial which just resorts the system call and return its least 24 significant bits.&lt;/p&gt;

&lt;p&gt;So how Redis evicts keys to reduce the memory usage? Now you don&amp;rsquo;t have a global LRU queue to eliminate the least recent used keys easily. Redis uses a eviction pool (actually a linked list) and populates it by some random keys, this pool is monotonous, the first node in the pool has the least idle time, the last one has the maximum idle time. Coming key will be inserted at the suitable position according to the idle time. For now, the pool size is hardcoded by 16. After this step, Redis will pick a best key from the end of pool and delete this key. This process will be repeated until the memory usage is under the limitation.&lt;/p&gt;

&lt;p&gt;The other way, LFU, is added to Redis newly, it traces each object&amp;rsquo;s access counter and evicts keys according the counter. You may think it&amp;rsquo;s naive and I just need to accumulate the counter whenever the object is touched, reducing the counter after a period. What&amp;rsquo;s amazing in Redis is that it just reuses the 24bits LRU clock space, so how can I maintain the counter and decay time (or the period) in just 3 octets? The way is neat and I have to admit that I cannot create this pattern, at least for now.&lt;/p&gt;

&lt;p&gt;Redis splits the 24 bits into 2 parts, the most 16 significant bits are used to store its last decay time; the least 8 significant bits are the space of counter, but are you kidding me? What can I exercise this poor 8 bits? Well, the counter, actually is a logarithmic counter, you know 2^255 is large enough and enough and enough, it&amp;rsquo;s not hard to guess the algorithm is not precise so much but as we all know Redis is pragmatic, so as long as it works well, it&amp;rsquo;s worthwhile. The counter updating logic is also probabilistic:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;uint8_t LFULogIncr(uint8_t counter) {
    if (counter == 255) return 255;
    double r = (double)rand()/RAND_MAX;
    double baseval = counter - LFU_INIT_VAL;
    if (baseval &amp;lt; 0) baseval = 0;
    double p = 1.0/(baseval*server.lfu_log_factor+1);
    if (r &amp;lt; p) counter++;
    return counter;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After glancing the above codes, you can easily see that the greater counter is, the smaller the probability to increse the counter. You can see the statistic in &lt;a href=&#34;[https://github.com/antirez/redis/blob/unstable/redis.conf#L1450](https://github.com/antirez/redis/blob/unstable/redis.conf#L1450)&#34;&gt;redis.conf&lt;/a&gt; to learn its performance.&lt;/p&gt;

&lt;p&gt;The decay time, determining the period that counter need to be havled. This is imperative otherwise the counter will converge to 255 and never regress, this ofcourse will influence the memory eviction mechanics.&lt;/p&gt;

&lt;p&gt;So which strategy should I use? You may need to ask yourself, what is my business? can I bear any data loss? If not, you even cannot use any memory eviction policy, the only choice is rejecting any write operations while memory exceeds the limitation. In contrast, if you are using Redis as data caching, memory policies have their use, LFU might be more concise, and if your Redis is new enough (&amp;gt;= 4.0), maybe you should try this! Or you can just use LRU, which is also good.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Internal of Redis Transaction</title>
      <link>https://tokers.github.io/posts/the-internal-of-redis-transcation/</link>
      <pubDate>Thu, 25 Apr 2019 16:43:18 +0800</pubDate>
      
      <guid>https://tokers.github.io/posts/the-internal-of-redis-transcation/</guid>
      <description>

&lt;p&gt;If you have the relational database background, you should be familiar with the transaction, execution of a transaction is atomic, which means either all the commands or none of them are performed.&lt;/p&gt;

&lt;p&gt;Although Redis is well-known as a NO-SQL typed database, it really supports the transaction facility long before. Now I&amp;rsquo;d like to dissect the essence of Redis transaction in this post, the content is based on a relatively new redis version (actually on my own branch which has the newest commit &lt;code&gt;5e8cac3&lt;/code&gt;), so the description might diverge from your expectation slightly.&lt;/p&gt;

&lt;h2 id=&#34;the-behavior&#34;&gt;The behavior&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s not surprising that there are only four transaction-relevant commands. They are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MULTI&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DISCARD&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EXEC&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WATCH&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;MULTI&lt;/code&gt; command is a trigger, which marks the current client into the transaction state when this command was deliverd by your Redis instance. After &lt;code&gt;MULTI&lt;/code&gt; command is handled, all the subsequent commands will be queued rather than being executed immediately until the &lt;code&gt;DISCARD&lt;/code&gt; or &lt;code&gt;EXEC&lt;/code&gt; command is occurred.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;DISCARD&lt;/code&gt; command, just respects its literal meaning, discards all the queued commands and cancels the client&amp;rsquo;s transaction state; The &lt;code&gt;EXEC&lt;/code&gt; command, just like the &lt;code&gt;COMMIT&lt;/code&gt; statement of SQL-based DBMS, notifies Redis to execute the transaction.&lt;/p&gt;

&lt;p&gt;But what about some of your commands are invalid? Say you just sent a command to your Redis server, if there is a syntactic error in your command (e.g. wrong number of arguments), Redis can detect it before queueing it, and error reply will be returned and this command wouldn&amp;rsquo;t be queued. Futhermore, current transaction will be marked as &amp;ldquo;dirty&amp;rdquo;, after the &lt;code&gt;EXEC&lt;/code&gt; command arrived, Redis just replied &amp;ldquo;-EXECABORT Transaction discarded because of previous errors.&amp;rdquo;, and current client&amp;rsquo;s transaction state is also discarded. These kind of errors, we call it &amp;ldquo;programming faults&amp;rdquo;, because it&amp;rsquo;s visible in your development environment and can be rectified easily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1:6379&amp;gt; MULTI
OK
127.0.0.1:6379&amp;gt; get a 1111
(error) ERR wrong number of arguments for &#39;get&#39; command
127.0.0.1:6379&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On the other hand, some invisible &amp;ldquo;runtime error&amp;rdquo; might arise in one of your commands, but only reported after the command is really executed, in such a case, command will be queued normally, because this command is right, at least in the syntax regard. When this command is being executed, Redis will detect this type of error and reply the corresponding message. But, what about the state of transaction? Does it abort? No, actually it will be executed without a break, Redis doesn&amp;rsquo;t &amp;ldquo;rollback&amp;rdquo; the transaction on this occasion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;127.0.0.1:6379&amp;gt; MULTI
OK
127.0.0.1:6379&amp;gt; INCR a
QUEUED
127.0.0.1:6379&amp;gt; HSET a b c
QUEUED
127.0.0.1:6379&amp;gt; EXEC
1) (integer) 2
2) (error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379&amp;gt; get a
&amp;quot;2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So what the hell is the &lt;code&gt;WATCH&lt;/code&gt;. You may know the optimistic locking mechanism, the &lt;code&gt;WATCH&lt;/code&gt; command is really an implementation of optimistic locking, it allows you to monitor some keys, if any of them is changed before the transaction is executed, then the transaction would be aborted, and it&amp;rsquo;s clients&amp;rsquo; responsibility to re-trigger the transaction.&lt;/p&gt;

&lt;h2 id=&#34;the-internal&#34;&gt;The internal&lt;/h2&gt;

&lt;p&gt;OK, so now we have digested the behavior of Redis transaction, it&amp;rsquo;s time to analysis its implementations!&lt;/p&gt;

&lt;p&gt;The source code is written in &lt;code&gt;multi.c&lt;/code&gt; with hundreds of lines. Let&amp;rsquo;s start from the &lt;code&gt;MULTI&lt;/code&gt; command, the command processor, &lt;code&gt;multiCommand&lt;/code&gt;, just ORed the &lt;code&gt;c-&amp;gt;flags&lt;/code&gt; with &lt;code&gt;CLIENT_MULTI&lt;/code&gt;. In addition, you cannot pass &lt;code&gt;MULTI&lt;/code&gt; recursively, or you would get the error reply.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s focus on the behavior of function &lt;code&gt;processCommand&lt;/code&gt; after the client into the transaction state, this is a high wrapper for the command calling. Looking through this function we find the last paragraph is related with transaction.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;if (c-&amp;gt;flags &amp;amp; CLIENT_MULTI &amp;amp;&amp;amp;
    c-&amp;gt;cmd-&amp;gt;proc != execCommand &amp;amp;&amp;amp; c-&amp;gt;cmd-&amp;gt;proc != discardCommand &amp;amp;&amp;amp;
    c-&amp;gt;cmd-&amp;gt;proc != multiCommand &amp;amp;&amp;amp; c-&amp;gt;cmd-&amp;gt;proc != watchCommand)
{
    queueMultiCommand(c);
    addReplay(c,shared.queued);
} else {
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Conditions are concise, just check whether the client is in the transaction state and after excluding some transaction related commands, our command will be queued. So let&amp;rsquo;s summarize the &lt;code&gt;queueMultiCommand&lt;/code&gt; by the way, it appends the current command to the &lt;code&gt;c-&amp;gt;mstate&lt;/code&gt;, which holds all the commands and their flags so far.&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s &lt;code&gt;DISCARD&lt;/code&gt;&amp;rsquo;s turn. The core part of &lt;code&gt;discardCommand&lt;/code&gt; is calling &lt;code&gt;discardTransaction&lt;/code&gt;, it frees all the queued commands and the relevant resouces, resets client&amp;rsquo;s transaction state and unwatches all the WATCHed keys (I will cover this in the later).&lt;/p&gt;

&lt;p&gt;The most important command, &lt;code&gt;EXEC&lt;/code&gt;, is implemented as the &lt;code&gt;execCommand&lt;/code&gt; function, before really executing all the queued commands, this function checks the current transaction&amp;rsquo;s state, like whether this transaction is dirty (due to some previous errors), if so, transaction will be aborted, just like the &lt;code&gt;DISCARD&lt;/code&gt; command&amp;rsquo;s behavior. Moreover, don&amp;rsquo;t forget the replication feature, if a transaction with some write operations is delivered in a Redis replica/slave instance, and the &lt;code&gt;replica-read-only&lt;/code&gt; option is enabled, transaction will be discarded too. Only all the checks passed, could the commands be run, in the meanwhile, commands will be propagated into the append only file buffer and all the replicas (with the prologue command &lt;code&gt;MULTI&lt;/code&gt; and epologue command &lt;code&gt;EXEC&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Wait a minute, it seems that we neglect the &lt;code&gt;WATCH&lt;/code&gt; command, right? The &lt;code&gt;WATCH&lt;/code&gt; command handler &lt;code&gt;watchCommand&lt;/code&gt;, is also simple enough, it builds the mapping between the target keys and the concerned clients (list) in &lt;code&gt;c-&amp;gt;db-&amp;gt;watched_keys&lt;/code&gt;. Once one of keys is modified, the corresponding client list will be iterated and each of them will be marked with the flag &lt;code&gt;CLIENT_DIRTY_CAS&lt;/code&gt;, which means this transaction now is dirty, and should be aborted while executing, this logic is boxed into the function &lt;code&gt;touchWatchedKey&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void touchWatchedKey(redisDb *db, robj *key) {
    list *clients;
    listIter li;
    listNode *ln;

    if (dictSize(db-&amp;gt;watched_keys) == 0) return;
    clients = dictFetchValue(db-&amp;gt;watched_keys, key);
    if (!clients) return;

    listRewind(clients, &amp;amp;li);
    while ((ln = listNext(&amp;amp;li))) {
        client *c = listNodeValue(ln);

        c-&amp;gt;flags |= CLIENT_DIRTY_CAS;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So this is the basic descriptions of Redis transaction, it&amp;rsquo;s atomic, simple, but doesn&amp;rsquo;t support rollback. Using it properly may help us but frankly I even don&amp;rsquo;t have a chance to use it in the production environment&amp;hellip; because the Lua script is preferable by constrast.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How is the message delivered in NSQD?</title>
      <link>https://tokers.github.io/posts/nsqd-message-delivery/</link>
      <pubDate>Fri, 15 Mar 2019 16:47:45 +0800</pubDate>
      
      <guid>https://tokers.github.io/posts/nsqd-message-delivery/</guid>
      <description>&lt;p&gt;NSQ is a well-known Message Queue System, and one of the most important component, NSQD, takes charge of the message delivery. It accepts the message from outer publishers passively, stashs them either in memory (a fixed-size Golang Channel), or persisting it in the disk. In the meanwhile, it waits consumers&amp;rsquo; arrival, pushes messages to these consumers as far as possible (tied with consumers&amp;rsquo; &lt;code&gt;RDY&lt;/code&gt; state).&lt;/p&gt;

&lt;p&gt;This post introduces the &amp;ldquo;footprint&amp;rdquo; of a message when it was published from a publisher to an NSQD instance, until it was eaten by some consumers, whose channels are different.&lt;/p&gt;

&lt;p&gt;Publishers can publish messages either through NSQD&amp;rsquo;s HTTP Restful APIs (&lt;code&gt;/pub&lt;/code&gt;, &lt;code&gt;/mpub&lt;/code&gt;), or over the TCP connection with NSQD&amp;rsquo;s client protocol (&lt;code&gt;PUB&lt;/code&gt;, &lt;code&gt;MPUB&lt;/code&gt; and &lt;code&gt;DPUB&lt;/code&gt; commands). Whatever way you chose, after you received the normal response, your message had been saved (but not so safe). In the publishers&amp;rsquo; side, messages are always tied with topic. Messages are always passed to &lt;strong&gt;topic&amp;rsquo;s&lt;/strong&gt; &lt;code&gt;gochan&lt;/code&gt; (or the disk queue if the memory channel is full) firstly, then they will be propagated to every channel, same put strategy is applied, actually each topic has such an exclusive goroutine to do this job (which is called &lt;code&gt;messagePump&lt;/code&gt; in NSQD&amp;rsquo;s code base), it receives messages from the memory &lt;code&gt;gochan&lt;/code&gt; or the backend &lt;code&gt;gochan&lt;/code&gt; (supported by the disk queue facility), and iterates each channel, deliveries each message to them.&lt;/p&gt;

&lt;p&gt;Consumers, however, is associated with the channel (not the Golang&amp;rsquo;s one). It subscribes a channel explicitly through the TCP protocol command &lt;code&gt;SUB&lt;/code&gt;. With the constant coordination of &lt;code&gt;RDY&lt;/code&gt; count, NSQD will push messages as far as possible, futhermore, once a message was sent to the client side, it would be saved in an &amp;ldquo;in-flight&amp;rdquo; queue, and there is some goroutines, named as &amp;ldquo;query scan workers&amp;rdquo;, always try to scan each topics&amp;rsquo; each channel, what they search are timed out messages, those passed to client but without an timely &lt;code&gt;FIN&lt;/code&gt; response, all of them would be re-pushed into the memory &lt;code&gt;gochan&lt;/code&gt; or the disk queue. This feature, ensures messages&amp;rsquo; safety in a way, although some risks still exist, like the NSQD instance was killed or the OS crashed, you can set the &lt;code&gt;-mem-queue-size&lt;/code&gt; option to &lt;code&gt;0&lt;/code&gt; to force NSQD puts messages in disk.&lt;/p&gt;

&lt;p&gt;The total process, is really clear and simple, but powerful. Tries to use NSQD under the proper circumstances, it will help you alot!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>哈夫曼编码在 HTTP/2 协议中的应用</title>
      <link>https://tokers.github.io/posts/huffman_in_hpack/</link>
      <pubDate>Sun, 19 Aug 2018 16:43:18 +0800</pubDate>
      
      <guid>https://tokers.github.io/posts/huffman_in_hpack/</guid>
      <description>&lt;p&gt;今天笔者想要简单讨论下哈夫曼编码在 HTTP/2 协议中的相关应用。本文不对 HTTP/2 协议内容进行展开，如果不曾了解过 HTTP/2 协议的话，请自行查阅相关文档。&lt;/p&gt;

&lt;p&gt;HPACK 是 HTTP/2 协议里用于头部压缩的一种技术，它又包括索引和编码两部分，其中索引是将 HTTP 头部替换为索引下标，编码则是将头部名、头部值进行哈夫曼编码，从而达到压缩头部大小，节省网络带宽和对端解析时的 CPU 消耗。&lt;/p&gt;

&lt;p&gt;哈夫曼编码依赖于哈夫曼树。哈夫曼树利用了字符集中每个字符的不同权重，“贪心”得构造出的一颗 01 二叉树，其中权重小的字符所在节点离树的根节点更远（其编码相对较长），权重大的字符所在的节点里树更节点更近（其编码相对较短）。关于哈夫曼树，更多的请查阅&lt;a href=&#34;https://en.wikipedia.org/wiki/Huffman_coding&#34;&gt;维基百科&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;HTTP/2 协议提供了 0-255 每个字符对应的哈夫曼编码（以及一个用于填充的字符，即 EOS，编码为 &lt;code&gt;0x3fffffff&lt;/code&gt;），这是通过分析大量 HTTP 头部的样本得到的。所有的 HTTP/2 协议实现都要按照这张表对 HTTP 头部进行编解码。&lt;/p&gt;

&lt;p&gt;如果了解了哈夫曼树的原理，那么再利用这张表构造出对应的哈夫曼树显得非常简单，解码时只需要在树上进行搜索即可，然而直接搜索的话，每次行进长度只有 1 个比特位，寻找一个编码长度为 N 的字符，时间复杂度是 O(N)。这并不是最优的方案，事实上，我们可以看到在 Nginx 和 nghttp2 的实现里，哈夫曼解码处理每次都是行进 4 个比特位，即寻找一个编码长度为 N 的字符，时间复杂度降到了 O(N / 4)，并且整棵哈夫曼树也被保存成一张表。&lt;/p&gt;

&lt;p&gt;每次行进 4 个比特位，实际上是将哈夫曼树扩展成为了一颗 16 叉树，因为每个节点都至多会有 16 个子节点。至于为什么是选择 4 个比特位而不是其他的，我想是因为考虑到编码时的便利性，因为刚好 4 个比特位是半个字节，我们可以通过简单的位运算得出 4 个比特位对应的值。然而此时问题也比之前更加复杂了一些，每次走 4 个比特位，我们需要记录下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;根据某一种走法走完 4 个比特位时，是否走到了非法的节点上（比如走到了 EOS 节点，或者途中某个节点不存在通往下个节点的路径）；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;走完这 4 个比特位时，是否经过了某个编码的字符（在哈夫曼树上则对应到叶子节点）；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当所有头部的解码结束时，是否停留在一个“合法的节点”上，所谓合法的节点，指的是在这个节点上刚好完成一次解码操作，或者当前的路径（从根节点到当前节点），刚好是 EOS 的前缀；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;不满足这些条件时，说明解码已经出错，对端没有按照协议标准对头部进行压缩。&lt;/p&gt;

&lt;p&gt;基于这些前提和条件，我用 Lua 语言写了一个生成哈夫曼解码表的&lt;a href=&#34;https://github.com/tokers/lua-resty-http2/blob/master/util/mkhuffdectbl&#34;&gt;程序&lt;/a&gt;，这为我实现 lua-resty-http2 奠定了基础，该部分参考了 Nginx 和 nghttp2。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ngx_lua 问题归档（1）</title>
      <link>https://tokers.github.io/posts/ngx-lua-drawback-record-1/</link>
      <pubDate>Sun, 05 Aug 2018 15:47:45 +0800</pubDate>
      
      <guid>https://tokers.github.io/posts/ngx-lua-drawback-record-1/</guid>
      <description>&lt;p&gt;前段时间在公司引流机器上偶然发现我们的 OpenResty/Nginx 服务存在进程崩溃的情况，错误日志里记录了如下的堆栈：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ngx_http_lua_ssl_cert_handler
tls_post_process_client_hello+0x1be)
ossl_statem_server_post_process_message
read_state_machine
state_machine
ossl_statem_accept
ssl3_read_bytes
ssl3_read_internal
ssl3_read
ssl_read_internal
SSL_read
ngx_ssl_recv
ngx_http_v2_read_handler
ngx_http_v2_idle_handler
ngx_epoll_process_events
ngx_process_events_and_timers
ngx_worker_process_cycle
ngx_spawn_process
ngx_start_worker_processes
ngx_master_process_cycle
main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（在崩溃时打印其堆栈的功能是我们自己加入到 Nginx 中的，官方版本并不支持。）&lt;/p&gt;

&lt;p&gt;从堆栈信息来看，当前正在处理的连接，HTTP 协议已经升级到了  HTTP/2 （&lt;code&gt;ngx_http_v2_idle_handler&lt;/code&gt;）。而从偏顶部的函数来看，当前连接似乎又进入到了 SSL/TLS 握手协议的处理，既然堆栈上出现了 HTTP/2 相关的函数，说明当前一定不是第一次 SSL/TLS 握手（否则 HTTP 协议不可能为 HTTP/2），这让我联想到 &lt;a href=&#34;https://devcentral.f5.com/articles/ssl-profiles-part-6-ssl-renegotiation&#34;&gt;SSL renegotation&lt;/a&gt;，由于没有 coredump 文件的支持，从堆栈信息里看出来的仅限于这么多。&lt;/p&gt;

&lt;p&gt;问题至此，便只能带着问题去看源码了。既然问题还和 HTTP/2 有关，那么我猜测问题可能出现在某些地方没有兼容好 HTTP/2 相关逻辑有关，经过查看 &lt;code&gt;ngx_http_lua_ssl_cert_handler&lt;/code&gt; 函数（这是实现 &lt;code&gt;ssl_certificate_by_lua&lt;/code&gt; 功能的基础函数）和 HTTP/2 相关的逻辑，我发现问题出在 &lt;code&gt;c-&amp;gt;data&lt;/code&gt; 这个成员上，在协议升级到 HTTP/2 后，函数 &lt;code&gt;ngx_htp_v2_init&lt;/code&gt;会被调用到，这个函数将 &lt;code&gt;c-&amp;gt;data&lt;/code&gt; 赋值为了 &lt;code&gt;ngx_http_v2_connection_t&lt;/code&gt; 的一个实例，而在通常（非 HTTP/2） 的处理下，&lt;code&gt;c-&amp;gt;data&lt;/code&gt; 则是 &lt;code&gt;ngx_http_connection_t&lt;/code&gt; 的实例，&lt;code&gt;ngx_http_lua_ssl_cert_handler&lt;/code&gt;并没有区分这一情况，导致将实际按 &lt;code&gt;ngx_http_v2_connection_t&lt;/code&gt; 布局的数据按 &lt;code&gt;ngx_http_connection_t&lt;/code&gt;的结构来操作，从而引发了 segmentation fault。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;void
ngx_http_v2_init(ngx_event_t *rev)
{
    ......
      
    c-&amp;gt;data = h2c; /* 赋值为 ngx_http_v2_connection_t 实例 */

    rev-&amp;gt;handler = ngx_http_v2_read_handler;
    c-&amp;gt;write-&amp;gt;handler = ngx_http_v2_write_handler;

    c-&amp;gt;idle = 1;

    ngx_http_v2_read_handler(rev);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int
ngx_http_lua_ssl_cert_handler(ngx_ssl_conn_t *ssl_conn, void *data)
{
    ......
    
    hc = c-&amp;gt;data; /* 此时应该是 h2c (ngx_http_v2_connection_t) = c-&amp;gt;data */
  
    ......
      
    r-&amp;gt;main_conf = hc-&amp;gt;conf_ctx-&amp;gt;main_conf; /* 可能导致 segmentation fault */
    r-&amp;gt;srv_conf = hc-&amp;gt;conf_ctx-&amp;gt;srv_conf;   /* 可能导致 segmentation fault */
    r-&amp;gt;loc_conf = hc-&amp;gt;conf_ctx-&amp;gt;loc_conf;   /* 可能导致 segmentation fault */
    
  ......
}
    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;原因似乎找到了，我尝试通过 openssl s_client 这个工具来复现问题，但是没有成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo R | openssl s_client -connect ip:port -alpn h2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;（这里键入 &amp;ldquo;R&amp;rdquo; 是为了触发 SSL renegotation。）&lt;/p&gt;

&lt;p&gt;我想可能遗漏了一些重要细节，通过 gdb 单步跟踪，我发现在执行到 &lt;code&gt;ngx_http_lua_ssl_cert_handler&lt;/code&gt; 时，并没有执行到上述逻辑，而是走到了以下的逻辑：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;int
ngx_http_lua_ssl_cert_handler(ngx_ssl_conn_t *ssl_conn, void *data)
{
    .......

    if (cctx &amp;amp;&amp;amp; cctx-&amp;gt;entered_cert_handler) {
        /* not the first time */

        if (cctx-&amp;gt;done) {
            ngx_log_debug1(NGX_LOG_DEBUG_HTTP, c-&amp;gt;log, 0,
                           &amp;quot;lua_certificate_by_lua: cert cb exit code: %d&amp;quot;,
                           cctx-&amp;gt;exit_code);

            dd(&amp;quot;lua ssl cert done, finally&amp;quot;);
            return cctx-&amp;gt;exit_code;
        }

        return -1;
    }
  
     .......
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因为早在第一次 SSL/TLS 握手时，&lt;code&gt;cctx-&amp;gt;entered_cert_handler&lt;/code&gt;就已经置为 1。所以第二次重新协商时，并没有处理我们期望的逻辑。&lt;/p&gt;

&lt;p&gt;反过来说，要出现这个问题，那么第一次 SSL/TLS 握手一定不能走到 &lt;code&gt;ngx_http_lua_ssl_cert_handler&lt;/code&gt; 的逻辑，这个函数实际设置在 OpenSSL 内部的 &lt;code&gt;s-&amp;gt;cert-&amp;gt;cert_cb&lt;/code&gt; 上，如果要该函数不被调用，说明第一次握手服务端没有发送证书链到客户端，即触发了SSL Session Reuse 机制。&lt;/p&gt;

&lt;p&gt;幸运的是，这仍然可以通过 openssl s_client 来验证猜测：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo R | openssl s_client -connect ip:port -reconnect -alpn h2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行后观察错误日志，发现进程果然崩溃，问题已经成功定位到！&lt;/p&gt;

&lt;p&gt;我已经将问题反馈到了 OpenResty 官方并提交了 &lt;a href=&#34;https://github.com/openresty/lua-nginx-module/pull/1355&#34;&gt;PR&lt;/a&gt; 进行修复。值得注意的是，Nginx/1.15.2 已经彻底避免了这个问题，见 &lt;a href=&#34;http://hg.nginx.org/nginx/rev/dcab86115261&#34;&gt;dcab86115261&lt;/a&gt;；另外，如果 OpenSSL 版本低于 1.1.0，也不会有这个问题。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://tokers.github.io/about/</link>
      <pubDate>Sat, 04 Aug 2018 15:52:20 +0800</pubDate>
      
      <guid>https://tokers.github.io/about/</guid>
      <description>

&lt;p&gt;Hello, I am Chao Zhang (aka Alex Zhang), I&amp;rsquo;m a System Development Engineer at UPYUN Inc. My Email is &lt;a href=&#34;mailto:zchao1995@gmail.com&#34;&gt;zchao1995@gmail.com&lt;/a&gt; and the github ID is &lt;a href=&#34;https://github.com/tokers&#34;&gt;tokers&lt;/a&gt;. Why &amp;ldquo;tokers&amp;rdquo;? Because the name &amp;ldquo;Alex&amp;rdquo; in github is a &amp;ldquo;critical resource&amp;rdquo;, and &amp;ldquo;tokers&amp;rdquo; is the nickname that I created (random? maybe) since I was in college and played the &lt;a href=&#34;https://icpc.baylor.edu/&#34;&gt;ACM/ICPC&lt;/a&gt; competitions.&lt;/p&gt;

&lt;p&gt;Currently I&amp;rsquo;m concentrating with CDN (Content Delivery Network), involves so
many technologies, like &lt;a href=&#34;https://www.nginx.com/&#34;&gt;Nginx&lt;/a&gt;, &lt;a href=&#34;https://openresty.org/en/&#34;&gt;OpenResty&lt;/a&gt;, &lt;a href=&#34;https://www.openssl.org/&#34;&gt;OpenSSL&lt;/a&gt;, &lt;a href=&#34;https://golang.org/&#34;&gt;Golang&lt;/a&gt; and so forth.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m an OpenResty contributor. Moreover, I&amp;rsquo;m passionate with those techs.&lt;/p&gt;

&lt;p&gt;This site is used as my tech blog, interesting things will be archived.&lt;/p&gt;

&lt;h1 id=&#34;my-own-projects&#34;&gt;My Own Projects&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tokers/zstd-nginx-module&#34;&gt;zstd-nginx-module&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These Nginx C modules are used for taking the power of &lt;a href=&#34;https://facebook.github.io/zstd/&#34;&gt;ZStandard&lt;/a&gt; to Nginx.&lt;/p&gt;

&lt;p&gt;There are two modules here, one is &amp;ldquo;static&amp;rdquo;, for serving the existing .zstd
file on the disk. And the other one, is used to compress data with the sreaming
way.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tokers/lua-io-nginx-module&#34;&gt;lua-io-nginx-module&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This Nginx C module provides the basic file operations APIs with a mechanism that never block Nginx&amp;rsquo;s event loop. For now, it leverages Nginx&amp;rsquo;s thread pool, I/O operations might be offloaded to one of the free threads, and current Lua coroutine (Light Thread) will be yield until the I/O operations is done, in the meantime, Nginx in turn processes other events.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth to mention that the cost time of a single I/O operation won&amp;rsquo;t be reduced, it was just transferred from the main thread (the one executes the event loop) to another exclusive thread. Indeed, the overhead might be a little higher, because of the extra tasks transferring, lock waiting, Lua coroutine resumption (and can only be resumed in the next event loop) and so forth. Nevertheless, after the offloading, the main thread doesn&amp;rsquo;t block due to the I/O operation, and this is the fundamental advantage compared with the native Lua I/O library.&lt;/p&gt;

&lt;p&gt;The APIs are similar with the Lua I/O library, but with the totally different internal implementations, it doesn&amp;rsquo;t use the stream file facilities in libc (but keep trying to be consistent with it), the buffer is maintained inside this module, and follows Cosocket&amp;rsquo;s internals.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tokers/lua-resty-requests&#34;&gt;lua-resty-requests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Yet Another HTTP library for OpenResty! This HTTP library is for human beings, just like Python Request, APIs are trivil but wonderful.&lt;/p&gt;

&lt;p&gt;It provides many features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HTTP/1.0, HTTP/1.1 and HTTP/2 (WIP).&lt;/li&gt;
&lt;li&gt;SSL/TLS support.&lt;/li&gt;
&lt;li&gt;Chunked data support.&lt;/li&gt;
&lt;li&gt;Convenient interfaces to support features like json, authorization and etc.&lt;/li&gt;
&lt;li&gt;Stream interfaces to read request body.&lt;/li&gt;
&lt;li&gt;HTTP/HTTPS proxy.&lt;/li&gt;
&lt;li&gt;Latency metrics.&lt;/li&gt;
&lt;li&gt;Session support.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;m still working on this project and will improve it continuously.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tokers/lua-resty-http2&#34;&gt;lua-resty-http2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The HTTP/2 Protocol (Client Side) Implementation for OpenResty.&lt;/p&gt;

&lt;p&gt;This pure Lua library implements the client side HTTP/2 protocol, but not all the details are covered, for example, the stream dependencies is maintained but never used.&lt;/p&gt;

&lt;p&gt;There are some inherent limitations which are not solved, however. For more
details, you can just read the README.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hmmm, I will replenish this list in the future. :)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://tokers.github.io/posts/redis-replication-handshake-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tokers.github.io/posts/redis-replication-handshake-flow/</guid>
      <description>&lt;p&gt;Replication is an essential part for the high availability. Redis has its replication implemenation so long before. The master instance sends its changes (wrapping commands with the RESP protocol) to all replicas from time to time.&lt;/p&gt;

&lt;p&gt;Before transferring data between the master and replica, both sides need to do the handshake, which is activated by the replica.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://tokers.github.io/posts/the-cooperation-between-nsqlookupd-and-nsqd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tokers.github.io/posts/the-cooperation-between-nsqlookupd-and-nsqd/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>